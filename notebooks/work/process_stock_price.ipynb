{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# BRONZE_PATH = os.environ.get(\"BRONZE_PATH\")\n",
    "# SILVER_PATH = os.environ.get(\"SILVER_PATH\")\n",
    "# MINIO_ACCESS_KEY = os.environ.get(\"MINIO_ACCESS_KEY\")\n",
    "# MINIO_SECRET_KEY = os.environ.get(\"MINIO_SECRET_KEY\")\n",
    "\n",
    "# bronze_path = f\"s3a://{BRONZE_PATH}/vnstock3/stock_quote_history_daily/\"\n",
    "# silver_path = f\"s3a://{SILVER_PATH}/vnstock3/daily_stock_prices/\"\n",
    "\n",
    "# endpoint_url = \"http://localhost:9000\"\n",
    "\n",
    "# bronze_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = f\"s3a://dev/data/bronze/vnstock3/stock_quote_history_daily/\"\n",
    "silver_path = f\"s3a://data/silver/vnstock3/daily_stock_prices/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4a6ba5dc82de:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DailyStockPriceProcessor</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f018584e430>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup Spark sesssion\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .appName(\"DailyStockPriceProcessor\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false'),\n",
       " ('spark.app.startTime', '1730802195058'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-core_2.12-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/hadoop-aws-3.3.2.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-storage-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-1.12.367.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/s3-2.18.41.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-bundle-1.11.1026.jar'),\n",
       " ('spark.hadoop.fs.s3a.access.key', 'minio'),\n",
       " ('spark.app.submitTime', '1730802194871'),\n",
       " ('spark.hadoop.fs.s3a.path.style.access', 'true'),\n",
       " ('spark.app.id', 'app-20241105102315-0002'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
       " ('spark.driver.port', '45267'),\n",
       " ('spark.app.name', 'DailyStockPriceProcessor'),\n",
       " ('spark.driver.host', '4a6ba5dc82de'),\n",
       " ('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.master', 'spark://spark-master:7077'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.jars',\n",
       "  'file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-core_2.12-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/hadoop-aws-3.3.2.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/delta-storage-2.2.0.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-1.12.367.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/s3-2.18.41.jar,file:///usr/local/spark-3.3.2-bin-hadoop3/jars/aws-java-sdk-bundle-1.11.1026.jar'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.hadoop.fs.s3a.endpoint', 'http://minio:9000'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/jovyan/work/spark-warehouse'),\n",
       " ('spark.hadoop.fs.s3a.secret.key', 'minio123'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.spark.sql.delta.catalog.DeltaCatalog'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://4a6ba5dc82de:45267/jars/hadoop-aws-3.3.2.jar,spark://4a6ba5dc82de:45267/jars/aws-java-sdk-1.12.367.jar,spark://4a6ba5dc82de:45267/jars/delta-storage-2.2.0.jar,spark://4a6ba5dc82de:45267/jars/aws-java-sdk-bundle-1.11.1026.jar,spark://4a6ba5dc82de:45267/jars/delta-core_2.12-2.2.0.jar,spark://4a6ba5dc82de:45267/jars/s3-2.18.41.jar')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name| Id|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "|Cathy|  3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a DataFrame\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Id\"])\n",
    "df\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 4, 4, 6, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.parallelize([1, 4, 2, 2, 3, 4])\n",
    "# 1) sử dụng hàm map() biến đổi rdd phía trên thành [2, 4, 6, 8]\n",
    "# Ví dụ\n",
    "rdd.map(lambda x: x * 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading from bucket: An error occurred while calling o153.load.\n",
      ": java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths\n",
      "\ts3a://dev/data/bronze/vnstock3/stock_quote_history_daily\n",
      "\ts3a://dev/data/bronze/vnstock3/symbol_exchange\n",
      "\ts3a://dev/data/bronze/vnstock3/symbol_industry\n",
      "\ts3a://dev/metadata\n",
      "\n",
      "If provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:191)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:169)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the S3 bucket path\n",
    "bucket_name = \"dev\"  # Replace with your actual bucket name\n",
    "bucket_path = f\"s3a://{bucket_name}/\"\n",
    "\n",
    "# Check if the bucket path is valid\n",
    "if not bucket_name:\n",
    "    raise ValueError(\"Bucket name is null or empty.\")\n",
    "\n",
    "# Read the contents of the S3 bucket\n",
    "try:\n",
    "    df = spark.read.format(\"text\").load(bucket_path)\n",
    "    # Show the contents\n",
    "    df.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from bucket: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_path = 's3a://dev/data/bronze/vnstock3/stock_quote_history_daily/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"open\", FloatType(), True),\n",
    "    StructField(\"high\", FloatType(), True),\n",
    "    StructField(\"low\", FloatType(), True),\n",
    "    StructField(\"close\", FloatType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"loaded_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Read the raw data from the bronze path in MinIO\n",
    "raw_df = spark.read.schema(schema).json(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+----+----+-----+------+------+--------------------+----------+\n",
      "|                time|open|high| low|close|volume|symbol|    loaded_timestamp|updated_at|\n",
      "+--------------------+----+----+----+-----+------+------+--------------------+----------+\n",
      "|+32560-08-20 00:0...| 3.5| 3.5| 3.5|  3.5|  1500|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32568-11-06 00:0...|3.56|3.56|3.56| 3.56|  3000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32574-04-29 00:0...|3.63|3.63|3.63| 3.63|  2100|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32579-10-20 00:0...| 3.7| 3.7| 3.7|  3.7|  2300|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32588-01-06 00:0...|3.76|3.76|3.76| 3.76|  1500|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32593-06-28 00:0...|3.83|3.83|3.83| 3.83|  7600|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32598-12-19 00:0...|3.89|3.89|3.89| 3.89|  1400|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32607-03-08 00:0...|3.96|3.96|3.96| 3.96|  1000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32612-08-28 00:0...|4.02|4.02|4.02| 4.02|  2000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32618-02-18 00:0...|4.09|4.09|4.09| 4.09|  2100|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32626-05-07 00:0...|4.15|4.15|4.15| 4.15|  1500|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32631-10-28 00:0...|4.22|4.22|4.22| 4.22|  1600|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32637-04-19 00:0...|4.09|4.09|4.09| 4.09|  1000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32650-12-27 00:0...|4.15|4.15|4.15| 4.15|  1500|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32656-06-18 00:0...|4.22|4.22|4.22| 4.22|  1500|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32664-09-04 00:0...|4.29|4.29|4.29| 4.29|  4200|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32670-02-25 00:0...|4.35|4.35|4.35| 4.35|  6000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32675-08-18 00:0...|4.42|4.42|4.42| 4.42|  5300|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32683-11-04 00:0...| 4.5| 4.5| 4.5|  4.5|  3000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "|+32689-04-26 00:0...|4.59|4.59|4.59| 4.59|  6000|   HAP|+56805-05-30 08:4...|2024-11-01|\n",
      "+--------------------+----+----+----+-----+------+------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- loaded_timestamp: timestamp (nullable = true)\n",
      " |-- updated_at: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1818063"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
